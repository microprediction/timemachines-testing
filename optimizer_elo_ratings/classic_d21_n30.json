{"name": ["shgo_slqsp_sobol_cube", "shgo_powell_sobol_cube", "shgo_nelder_sobol_cube", "shgo_dogleg_sobol_cube", "scipy_slqsp_cube", "scipy_powell_cube", "scipy_nelder_cube", "scipy_dogleg_cube", "pysot_ei_cube", "pysot_lcb_cube", "pysot_random_cube", "pysot_srbf_cube", "pysot_dycors_cube", "ax_default_cube", "optuna_cmaes_cube", "optuna_tpe_cube", "optuna_random_cube", "platypus_genetic_cube", "platypus_evolutionary_cube", "platypus_nsgaii_cube", "platypus_nsgaii_cube", "platypus_cmaes_cube", "platypus_gde3_cube", "platypus_ibea_cube", "platypus_moead_cube", "platypus_omopso_cube", "platypus_smpso_cube", "platypus_spea2_cube", "platypus_epsmoea_cube", "nevergrad_ngopt_cube", "nevergrad_ngopt4_cube", "nevergrad_ngopt8_cube", "nevergrad_de_cube", "nevergrad_portfolio_cube", "nevergrad_oneplus_cube", "nevergrad_cma_cube", "nevergrad_hammersley_cube", "swarmlib_pso_cube", "optuna_random_cube_clone", "optuna_random_cube_clone_1", "optuna_random_cube_clone_2", "hyperopt_atpe_cube", "hyperopt_rand_cube", "hyperopt_tpe_cube", "pymoo_nsga2_cube", "pymoo_nelder_cube", "pymoo_nsga3_cube", "pymoo_unsga3_cube", "pymoo_pattern_cube", "pymoo_brkga_cube", "pymoo_nsga2_cube", "skopt_gp_default_cube"], "count": [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0], "rating": [1600, 1600, 1600.0, 1600, 1600, 1600, 1600, 1600, 1570.0, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600], "traceback": ["passing", "not yet run", "passing", "passing", "passing", "passing", "not yet run", "Optimizer was naughty playing as white. It used 100 evaluations when instructed to use 30", "passing", "not yet run", "not yet run", "not yet run", "passing", "not yet run", "not yet run", "not yet run", "passing", "not yet run", "not yet run", "Optimizer was naughty when playing black. It used 100 evaluations when instructed to use 27", "not yet run", "passing", "not yet run", "passing", "passing", "not yet run", "not yet run", "not yet run", "not yet run", "not yet run", "not yet run", "not yet run", "passing", "passing", "not yet run", "not yet run", "not yet run", "passing", "not yet run", "not yet run", "passing", "not yet run", "not yet run", "passing", "passing", "not yet run", "not yet run", "not yet run", "passing", "passing", "not yet run", "Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/timemachines/optimizers/eloratings.py\", line 41, in optimizer_game\n    white_best_val, white_best_x, white_feval_count = white(objective, n_trials=n_white_trials, n_dim=n_dim,\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/timemachines/optimizers/skoptcube.py\", line 59, in skopt_gp_default_cube\n    return skopt_gp_cube_factory(objective=objective,n_trials=n_trials,n_dim=n_dim, with_count=with_count,\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/timemachines/optimizers/skoptcube.py\", line 50, in skopt_gp_cube_factory\n    result = gp_minimize(func=_objective,  dimensions=bounds, n_calls=n_trials, n_jobs=1, **gp_params)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/skopt/optimizer/gp.py\", line 259, in gp_minimize\n    return base_minimize(\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/skopt/optimizer/base.py\", line 302, in base_minimize\n    result = optimizer.tell(next_x, next_y)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/skopt/optimizer/optimizer.py\", line 493, in tell\n    return self._tell(x, y, fit=fit)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/skopt/optimizer/optimizer.py\", line 536, in _tell\n    est.fit(self.space.transform(self.Xi), self.yi)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/skopt/learning/gaussian_process/gpr.py\", line 195, in fit\n    super(GaussianProcessRegressor, self).fit(X, y)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\", line 235, in fit\n    optima = [(self._constrained_optimization(obj_func,\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\", line 506, in _constrained_optimization\n    opt_res = scipy.optimize.minimize(\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 619, in minimize\n    return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\", line 306, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 261, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 136, in __init__\n    self._update_fun()\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 226, in _update_fun\n    self._update_fun_impl()\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 133, in update_fun\n    self.f = fun_wrapped(self.x)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 130, in fun_wrapped\n    return fun(x, *args)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 74, in __call__\n    self._compute_if_needed(x, *args)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/optimize/optimize.py\", line 68, in _compute_if_needed\n    fg = self.fun(x, *args)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\", line 227, in obj_func\n    lml, grad = self.log_marginal_likelihood(\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\", line 481, in log_marginal_likelihood\n    alpha = cho_solve((L, True), y_train)  # Line 3\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/scipy/linalg/decomp_cholesky.py\", line 194, in cho_solve\n    b1 = asarray_chkfinite(b)\n  File \"/opt/hostedtoolcache/Python/3.8.7/x64/lib/python3.8/site-packages/numpy/lib/function_base.py\", line 488, in asarray_chkfinite\n    raise ValueError(\nValueError: array must not contain infs or NaNs\n"], "active": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true]}